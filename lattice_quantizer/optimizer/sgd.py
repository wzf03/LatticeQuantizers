from typing import Callable, Tuple

import numpy as np
from numba import float64, int32, int64, njit, types
from tensorboardX import SummaryWriter
from tqdm import tqdm

from lattice_quantizer.algorithm import closest_lattice_point as clp
from lattice_quantizer.algorithm import lattice_basis_reduction as lbr
from lattice_quantizer.lr_scheduler import LRScheduler, RatioLR


@njit(float64[:, :](types.npy_rng, int32), inline="always")
def _gran(rng: np.random.Generator, n: int) -> np.ndarray:
    """
    GRAN returns an n * m matrix of random independent real numbers,
    each with a Gaussian zero-mean, unit-variance distribution. We use the
    Gaussian random number generator by Paley and Wiener
    """
    return rng.normal(0, 1, (n, n))


@njit(float64[:, :](types.npy_rng, int32, int32), inline="always")
def _uran(rng: np.random.Generator, n: int, batch: int = 1) -> np.ndarray:
    """
    URAN (n) returns n random real numbers, which are uniformly distributed
    in [0, 1) and independent of each other. Interpreted as a vector, URAN (n)
    returns a random point uniformly distributed in an n-dimensional hypercube.
    We use the permuted congruential generator, which is well documented
    and fulfills advanced tests of randomness.
    """
    return rng.random((batch, n))


@njit(int64[:](float64[:, :], float64[:]), inline="always")
def _clp(generator: np.ndarray, r: np.ndarray) -> np.ndarray:
    """
    The closest lattice point function CLP (B, x) finds the point in the lattice
    generated by B that is closest to x. The output is not the lattice point
    itself but rather its integer coordinates $\\argmin_{u \in Z^n} || x - uB ||^2$.

    We use [22, Algorithm 5], which is the fastest general closest-point search
    algorithm known to us. It applies to square, lower-triangular generator matrices
    with positive diagonal elements, which is exactly how lattices are represented
    in Algorithm 1.
    """
    return clp.closest_lattice_point(generator, r)


@njit(float64[:, :](float64[:, :]), inline="always")
def _orth(x: np.ndarray) -> np.ndarray:
    """
    The orthogonal transformation function ORTH (B) rotates and reflects an
    arbitrary generator matrix into a square, lowertriangular form with positive
    diagonal elements. This corresponds to finding a new coordinate system for
    the lattice, in which the first i unit vectors span the subspace of the
    first i basis vectors, for i = 1, . . . , n. We implement this function by
    Cholesky-decomposing [27, Sec. 4.2] the Gram matrix A = BBT. In the context
    of (2), orthogonal transformation corresponds to right-multiplying the
    generator matrix by a semiorthogonal matrix R.
    """
    return np.linalg.cholesky(x @ x.T)


@njit(float64[:, :](float64[:, :]), inline="always")
def _red(basis: np.ndarray) -> np.ndarray:
    """
    The reduction function RED(B) returns another generator matrix for the
    lattice generated by B, in which the rows (basis vectors) are shorter and
    more orthogonal to each other than in B. If no improved generator matrix is
    found, B is returned unchanged. A fast and popular algorithm for the purpose
    is the Lenstra-Lenstra-Lov Ìasz algorithm [24, Fig. 1], which we apply in
    this work. In the context of (2), reduction corresponds to finding a suitable U .
    """
    return lbr.lattice_basis_reduction(basis)


@njit(cache=True)
def _init_basis(rng: np.random.Generator, n: int) -> np.ndarray:
    basis = _orth(_red(_gran(rng, n)))
    volume = np.prod(np.diag(basis))
    return (volume ** (-1 / n)) * basis


@njit(inline="always")
def _forward(
    z: np.ndarray,
    basis: np.ndarray,
) -> Tuple[np.ndarray, np.ndarray]:
    y_batch = np.empty_like(z)
    e_batch = np.empty_like(z)

    for i in range(z.shape[0]):
        y_batch[i] = z[i] - _clp(basis, z[i] @ basis)
        e_batch[i] = y_batch[i] @ basis

    return y_batch, e_batch


@njit(inline="always")
def _backward(
    y: np.ndarray,
    e: np.ndarray,
    basis: np.ndarray,
) -> np.ndarray:
    gradient = np.zeros_like(basis)
    n = basis.shape[0]
    for i in range(y.shape[0]):
        for j in range(n):
            for k in range(j):
                gradient[j, k] += y[i, j] * e[i, k]
            gradient[j, j] += y[i, j] * e[i, j] - (
                np.sum(e[i] ** 2) / (n * basis[j, j])
            )
    return gradient


@njit(cache=True)
def _step(
    n: int,
    basis: np.ndarray,
    rng: np.random.Generator,
    lr: float,
    batch_size: int,
) -> float:
    # Generate data
    batch_x = _uran(rng, n, batch_size)

    # Forward and backward pass
    batch_y, batch_e = _forward(batch_x, basis)

    volume = np.prod(np.diag(basis))
    loss = np.sum(batch_e**2) / batch_size / (n * volume ** (2 / n))

    gradient = _backward(batch_y, batch_e, basis)

    # Update basis
    basis[:] -= lr * (gradient / batch_size)

    return loss


@njit(cache=True)
def _step_n(
    steps: int,
    n: int,
    basis: np.ndarray,
    t: int,
    rng: np.random.Generator,
    reduction_interval: int,
    batch_size: int,
    lr_scheduler: Callable[[int], float],
) -> float:
    nsm = 0.0
    for _ in range(steps):
        lr = lr_scheduler(t)
        loss = _step(n, basis, rng, lr, batch_size)
        nsm += 1 / steps * loss

        if t % reduction_interval == reduction_interval - 1:
            basis[:] = _orth(_red(basis))
            volume = np.prod(np.diag(basis))
            basis[:] = (volume ** (-1 / n)) * basis
        t += 1
    return nsm


DEFAULT_LR_SCHEDULER = RatioLR(0.001, ratio=500)


class SGDLatticeQuantizerOptimizer:
    def __init__(
        self,
        steps: int = 10_000_000,
        reduction_interval: int = 100,
        lr_scheduler: LRScheduler = DEFAULT_LR_SCHEDULER,
        batch_size: int = 8,
        log_interval: int = 10000,
        log_dir: str = "logs",
    ):
        self.steps = steps
        self.reduction_interval = reduction_interval
        self.log_interval = log_interval
        self.batch_size = batch_size
        self.lr_scheduler = lr_scheduler

        self.rng = np.random.default_rng()
        self.log_dir = log_dir

    def optimize(self, dimension: int) -> np.ndarray:
        basis = _init_basis(self.rng, dimension)
        t = 0
        lr_scheduler = self.lr_scheduler.get_lr_scheduler(self.steps)
        pbar = tqdm(total=self.steps, desc="Optimizing")
        writer = SummaryWriter(log_dir=self.log_dir)

        for i in range(0, self.steps, self.log_interval):
            steps = min(self.log_interval, self.steps - i)
            nsm = _step_n(
                steps=steps,
                n=dimension,
                basis=basis,
                t=t,
                rng=self.rng,
                reduction_interval=self.reduction_interval,
                batch_size=self.batch_size,
                lr_scheduler=lr_scheduler,
            )
            pbar.update(steps)
            pbar.set_postfix({"nsm": f"{nsm:.4f}"})
            t += steps

            writer.add_scalar("train/nsm", nsm, i)

        writer.close()
        return basis
